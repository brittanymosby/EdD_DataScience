---
title: "Classification"
author: "LLO 8200"
output: html_document
---

Classification is the process of predicting discrete group membership. Understanding which individuals are likely to be members of which groups is a key task for data scientists. For instance, most recommendation engines that are at the heart of consumer web sites are based on classification algorithms, predicting which consumers are likely to purchase which products. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(knitr)
library(modelr)
library(caret)
library(forcats)
```

# Random Acts of Pizza

Today we'll be working with the pizza dataset, which comes from the subreddit [r/Random Acts of Pizza](https://www.reddit.com/r/Random_Acts_Of_Pizza/). Each line represents a post requesting users to send them pizza. We have various characteristics of these posts, along with the request text from the post itself. We'll use these characteristics as independent variables to predict whether or not the poster actually received pizza. 

This lesson is inspired by [this article](http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/download/8106/8101)

```{r data}
load("za_train.RData")
```

Our goal is to create a classifier that will accurately classify posts in a testing dataset as to whether it will receive a pizza or not, based on the content of the post. This is a VERY common task in data science-- taking user supplied content and using it to accurately classify that user, typically as someone who will buy a product or service.   
 
## Dependent Variable
The dependent variable is a binary variable, `got_pizza` that is coded "0" if the user did not get a pizza after posting and "1" if the user reported getting pizza. Let's take a look at this and see how many people posted that they got a pizza.
 
```{r}
table(za_train$got_pizza)
```

This tells us the raw numbers. Lots of times we want to know the proportions. The function `prop.table` can do this for us. 

```{r}
prop.table(table(za_train$got_pizza))
```
 
So, `r  prop.table(table(za_train$got_pizza))[2]` of posts indicate that they were sent a pizza as a result of their post. Now, we're interested in taking information in the posts themselves to see what makes it more or less likely that they would indicate that they received a pizza. 
 

## Conditional Means as a Classifier

We'll start by generating some cross tabulations and some quick plots, showing the probability of receiving pizza according to several characteristics of the post.  We start with a basic cross tab of the dependent variable. We use `prop.table` to change this from raw counts to proportions. I also provide a brief example of how to do a nice table for Markdown using the `kable` function. 

```{r descriptives}
#Cross Tabs

za_train%>%
  count(got_pizza)%>% # Count numbers getting pizza
  mutate(p=prop.table(n))%>% #mutate for proportions using prop.table
  kable(format="markdown") # output to table

```

So, about 75% of the sample didn't get pizza, about 25% did. 

Next, we cross-tabulate receiving pizza with certain terms. First, if the request mentioned the word "student."

```{r}
prop.table(table(za_train$student,za_train$got_pizza),margin=1)
```

Next, if the request mentioned the word "grateful."

```{r}
g_table<-table(za_train$grateful,za_train$got_pizza);g_table

prop.table(g_table,margin=1)
```
*Question: would you recommend adding "grateful" to a post requesting pizza? Why or why not?*

Cross tabs using binary data are equivalent to generating conditional means for continuous data, as shown below. 
```{r condtional_means}
#Predictions using conditional means

za_train%>%group_by(grateful)%>%summarize(mean(got_pizza))

```

Note how the mean of `got pizza` is equivalent to the proportion of "1" responses in the previous table. 

But, we can also use conditional means to get proportions for very particular sets of characteristics. In this case, what about individuals who included some combination of the terms "grateful", "student", and "poor" in their posts? 

```{r}
za_sum<-za_train%>%
  group_by(grateful,student,poor)%>%
  summarize(mean_pizza=mean(got_pizza))%>%
  arrange(-mean_pizza)

za_sum%>%kable()
```

The initial evidence here makes it look like the posts that included the terms "grateful" and "student" had the highest probability of receiving a pizza (or at least posting that they received a pizza!).

```{r graphing_conditional_means}
gg<-ggplot(za_sum,aes(x=grateful,y=mean_pizza,fill=grateful))
gg<-gg+geom_bar(stat="identity")
gg<-gg+facet_wrap(~student+poor)
gg
```

## Classifiation Using Linear Probability Model

We can use standard OLS regression for classification. It's not ideal, but most of the time it's actually not too bad, either. Below we model the binary outcome of receiving pizza as a function of karma, total posts, posts on the pizza subreddit, and whether or not the poster mentioned the words "student" or "grateful."

```{r linear_model}
# Linear model
lm_mod<-lm(got_pizza~
             karma+
             total_posts+
             raop_posts+
             student+
             grateful,
           data=za_train,y=TRUE);summary(lm_mod)
```

We're going to do something a bit different with the predictions from this model. After creating predictions, we're going to classify everyone with a predicted probability above .5 as being predicted to get a pizza, while everyone with a predicted probability below .5 is predicted to not get one. We'll then compare our classifications with the actual data. 

```{r predictions}
za_train<-za_train%>%
  add_predictions(lm_mod)%>% ## Add in predictions from the model
  rename(pred_lm=pred)%>% ## rename to be predictions from ols (lm)
  mutate(pred_lm_out=ifelse(pred_lm>=.5,1,0)) ## Create predicted classification, 1 if above .5
```

Let's create a table that shows the predictions of our model against what actually happened
```{r}
pred_table<-table(za_train$got_pizza,za_train$pred_lm_out)
rownames(pred_table)<-c("Predicted 0","Predicted 1")
colnames(pred_table)<-c("Actually 0","Actually 1")

pred_table

prop.table(pred_table)
```

The confusion matrix (or error matrix) is a set of measures that can be used to assess the model fit of a classifier. The terms below are commonly used in classification and prediction, and can be thought of as marginal probabilities from the 2X2 contingency table of predictions and actual results: true positive, false positive, true negative, and false negative. (N.B. recall from applied stats that a false positive is equivalent to a Type I error, and a false negative is the same as a Type II error.)

- **Accuracy**: Percentage of correctly classified cases overall. $$\frac{TP+TN}{total}$$
- **No information rate (NIR)**: the highest accuracy possible if every case is classified as the largest class. In general, a good model will have an accuracy that is greater than the no information rate.
- **Positive prediction value (PPV)**: Also called the *precision*, the percentage of correctly classified positive cases, given a positive prediction. $$\frac{TP}{TP+FP}$$
- **Negative prediction value (NPV)**: Percentage of correctly classified negative cases, given a negative prediction. $$\frac{TN}{TN+FN}$$
- **Sensitivity**: Also referred to as the *recall* or *true positive rate*, the percentage of correctly classified positive cases, given the case was actually positive. $$\frac{TP}{TP+FN}$$
- **Specificity**: Also called the *selectivity* or *true negative rate*, the percentage of correctly classified negative cases, given the case was actually negative. $$\frac{TN}{TN+FP}$$
- **Balanced accuracy**: the mean of sensitivity and specificity.

```{r}
confusionMatrix(as_factor(za_train$got_pizza),as_factor(za_train$pred_lm_out), positive = "1")
```

We're usually interested in three things: the overall accuracy of a classification is the proportion of cases accurately classified. The sensitivity is the proportion of "ones" that are accurately classified as ones-- it's the probability that a case classified as positive will indeed be positive. Specificity is the probability that a case classified as 0 will indeed by classified as 0. 

*Question: how do you get perfect specificity? How do you get perfect sensitivity?*

## Logistic Regression for a better prediction
There are several well-known problems with linear regression as a classification algorithm. Two should give us pause: it can generate probabilities outside of [0,1] and it implies a linear change in probabilities as a function of the predictors which may not be justified given the underlying relationship between the predictors and the probability that the outcome is 1. Logistic regression should give a better predicted probability, one that's more sensitive to the actual relationship between the predictors and the outcome. Logistic regression is set up to handle binary outcomes as the dependent variable. In particular, the predictions will always be a probability, which makes it better than the ironically named linear probability model.

In logistic regression we are estimating the probability an event occurs, given a linear combination of independent variables. This works best for discrete, unordered dependent variables. It is similar in procedure to linear regression, and can be used as the first step in a classification algorithm. The benefit of using a logistic model rather than linear is based on its mathematical properties:

*A review of probability and odds*: Consider a random event with one of two outcomes where $$P(Y=1) = p$$ and $$P(Y=0) = q \text{ or } 1-p$$
Then the **odds for** the event is the ratio $$p/q \text{ or } p:q$$
This will always be a number greater than 0. Example: The probability of rain on a given day is 80%. What are the odds of rain? Answer- $$\frac{.80}{.20}=\frac{4}{1}$$ or also displayed as 4:1, read "four-to-one."

The **log-odds** are the natural logarithm $\ln(x)$ of the odds of an event: $\ln(p/q)$. For an event with two equally likely outcomes, the odds are 1:1 and the log-odds are 0. If the event is more likely to NOT occur, the odds will be a fraction, and the log-odds will be negative. If the event is more likely TO occur, the odds will be greater than 1, and the log-odds will be positive. Mathematically, this allows us to map the probability space [0,1] to any real number, which is particularly useful for classifying a binary event based on a set of independent variables which may be discrete or continuous.

Similar to linear regression, logistic regression is the process of finding coefficients $\beta$ for the logistic expression $$\ln(p/q) = \beta_0
+ \sum(\beta_i x_i)$$ where p and q are defined as above. This equation describes the linear relationship between the independent variables and the **log-odds of the dependent variable**.
See [here](https://en.wikipedia.org/wiki/Logistic_regression#Probability_of_passing_an_exam_versus_hours_of_study) for an interesting example of how logistic regression can be used to predict the probability of passing an exam, based on hours of study.

The downside to logistic regression is that it is modeling the log odds of the outcome, which means all of the coefficients are expressed as log odds, which no one understands intuitively. In this class, we're going to concentrate on logistic regression's ability to produce probabilities as predictions. Below I run the same model using logistic regression. Note the use of `glm` and the `family` option, which specifies a functional form and a particular link function. 

```{r}
#Logistic model

logit_mod<-glm(got_pizza~
             karma+
             total_posts+
             raop_posts+
             student+
             grateful,
             data=za_train,
            na.action=na.exclude,
            family=binomial(link="logit"),
               y=TRUE)

summary(logit_mod)
```

Logistic regression returns a set of log-odds coefficients that can be converted back to valid probabilities for each observation (unlike linear regression, which could possibly give values outside of the probability space [0,1]). With these results in hand we can generate predicted probabilities and see if this model did any better in classifying posts. To get predicted probabilities, we need to specify `type=response` in our prediction call. 

```{r}
za_train<-za_train%>%
  mutate(pred_logit=predict(logit_mod,type="response"))
```

We can convert the prediction probabilities to a binary variable by setting a "threshold" of .5. Any prediction above .5 is considered to be a 1, anything below, a 0.

```{r}
za_train<-za_train%>%
    mutate(pred_logit_out=ifelse(pred_logit>=.5,1,0))
```

Now we create a confusion matrix to see how we did. 
```{r}
confusionMatrix(data=as_factor(za_train$pred_logit_out),reference=as_factor(za_train$got_pizza), positive = "1")
```

Overall, based on the independent variables of Reddit karma, total posts, posts in the pizza subreddit, if the post includes "student", and if it includes "grateful," our logistic classifier has 75.6% accuracy. This may seem promising, however: once we compare to the NIR, we see that we can get almost the same accuracy by classifying every case as 0 or did not get pizza. The p-value is 0.3732 for a hypothesis test with alternative hypothesis $$H_1: ACC>NIR$$ indicating there is not sufficient evidence to support the conclusion that the accuracy for this classifier is greater than the no information rate.

Additionally, examining the PPV and NPV reveals that this classifier has better negative predictive power (75.8%) compared to positive predictive power (61.1%). Meaning this classifier is slightly better at identifying posts that did not get pizza: 75.8% of the time the classifier predicted no pizza, it was correct. The sensitivity shows 3% of posts that actually got pizza were correctly classified; whereas 99.3% of posts that did not get pizza were correctly classified, again pointing to the greater confidence in negative predictions over positive ones.

## Applying predictions to the testing dataset

With our new (not very good) classifier, we can now add predictions to the testing dataset, and see how good this classifier is at predicting out of sample information. 

```{r}
load("za_test.RData")

za_test<-za_test%>%
  mutate(pred_logit=predict(logit_mod,newdata=.,type="response"))%>%
      mutate(pred_logit_out=ifelse(pred_logit>=.5,1,0))

confusionMatrix(data=as_factor(za_test$pred_logit_out),reference=as_factor(za_test$got_pizza), positive = "1")
```

**Question** Examine the confusion matrix, how did the classifier perform on the test data?

In all, we would conclude that this model (the logistic regression with the five specified independent variables) is NOT a good fit for the data.

Other post-hoc measures of model fit that you may see in literature and practice include:

- False discovery rate is the probability of false positive result out of all predicted positive cases. Equivalent to 1-PPV.
- False omission rate is the probability of false negative result out of all predicted negative cases. Equivalent to 1-NPV.
- False negative or miss rate is the probability of a false negative out of all actually positive cases. Equivalent to 1-sensitivity.
- False positive or false alarm rate is the probability of a false positive out of all the actually negative cases. Equivalent to 1-specificity.

## Thinking about classifiers

First, make sure that your dependent variable really is binary. If you're working with a continuous variable (say, income) don't turn it into a binary variable (e.g. low income). 

Second, remember that classifiers must also balance sensitivity and specificity. Don't be overly impressed by a high overall percent correctly predicted, nor a high level of either specificity or sensitivity. Instead, look for classifiers that have both.