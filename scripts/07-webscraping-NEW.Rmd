---
title: "Web Scraping and APIs"
author: "BL Mosby"
output: html_document
---

## Introduction

Many large web sites host a huge amount of information. This information is encoded and delivered on demand to the user within a web page, which is really just a markup language that a browser can understand. We can take this data and analyze it using R, via a variety of different means. Today we'll cover scraping web tables and interacting via Automated Programming Interfaces.

## Ethics and Responsiblity

Many of the tools we'll cover can be quite powerful ways to interact with data stored online and gather it for analysis. Because they're powerful, you need to be careful with them. In particular, try to request information in a way that will not burden the website owners. What constitutes a burden depends on the website. Google, Twitter, Facebook, all of the big websites have protections in place to guard against too many requests and have a huge amount of capacity for taking the requests they grant. Smaller websites may not have either. Always strive to be minimally intrusive: you're usually getting this data for free. 

## Ways of getting data from the web

We will cover several different ways you can get data from the web

1. Directly downloading web pages via the `url()` command. 
2. Scraping simple web tables via `read_html()` and `html_table()` command
3. Interacting with Application Programming Interfaces (APIs) via R libraries that have been designed as "wrappers" for these interfaces, like the awesome `acs` library and the `tigris` library for geographic shapes. 
4. Interacting with APIs directly.


## Libraries

We will use multiple new libraries today. Among the ones you'll need: 
* `rvest` for scraping websites
* `acs` for accessing American Community Survey data via the census API

```{r}
library(tidyverse)
library(rvest)
library(acs)
library(lubridate)
library(noncensus) #this package can be found in the Archive
library(tigris)
```


## API keys

You will also need an API key. 
* The Census API, available here: https://api.census.gov/data/key_signup.html 


# Basics of interacting with information stored online
R can understand a web connection via the `url` command. Once that connection is established, we can download whatever we'd like. 

```{r}
#Web connections: url

r_home = url("http://www.r-project.org/")
r_home
```


```{r}
# Pulling text from a website using `readlines`

# url of Moby Dick (project Gutenberg)
moby_url = url("http://www.gutenberg.org/files/2701/2701-h/2701-h.htm")

# reading the content (first 1500 lines)
moby_dick = readLines(moby_url, n = 1500)

#displays lines 1205 through 1220
moby_dick[1205:1220]
```



# Scraping web tables

When we talk about "scraping" a web table, we're talking about pulling a table that exists on a website and turning it into a usable data frame for analysis. Below, I take the table from  `http://en.wikipedia.org/wiki/Marathon_world_record_progression` for men's marathon times and plot the change in speed in m/s as a function of the date that the world record was set. 

```{r}
marathon_wiki = "https://en.wikipedia.org/wiki/Marathon_world_record_progression"

#Get the page, pull the tables via html_table
marathon <- read_html(marathon_wiki)%>%html_table(fill=TRUE)

#Men's is the first table
marathon<-tibble(data.frame(marathon[[1]]))

#Convert time to seconds
marathon<-marathon%>%
  mutate(Time2=hms(as.character(Time)))%>%
  mutate(Time2=period_to_seconds(Time2))

#Marathons are 42,200 meters long
marathon$speed<-(4.22e4)/marathon$Time2

#Get dates in a standard yyyy-mm-dd format using Lubridate package
marathon$date<-mdy(marathon$Date)

head(marathon)
```


## Progression of World Record Marathon Speed in Meters/Second
```{r}
marathon<-marathon%>%mutate(Nationality = 
                              fct_reorder(.f=as.factor(Nationality),.x=-speed, .fun=max)) 
#Reorder nationality by fastest times
```


```{r}
#create a plot of speed over time, with Nationality specified by color

g1<-ggplot(data=marathon, aes(y=speed,x=date,color=Nationality))
g1<-g1+geom_point()+
           xlab("Date")+
           ylab("Meters/Second")
g1
```

_Quick Exercise_ Repeat the above analysis for women's world record progression.

---

# Interacting via APIs

Many websites have created Application Programming Interfaces, which allow the user to directly communicate with the website's underlying database without dealing with the intermediary web content. These have been expanding rapidly and are one of the most exciting areas of development in data access for data science. 

Today, we'll be working with the American Community Survey from the census. Please go to: `http://www.census.gov/developers/` and click on "Get a Key" to get your census key. 

*YOU NEED TO PAY ATTENTION TO TERMS OF USE WHEN USING APIS. DO NOT VIOLATE THESE.*

With these keys in hand, we can interact with these various databases. Let's say we have information on zip codes for students, and we want to know their likely income level. We can do this by using the American Community Survey API. 

## Zip Code Level Data from the American Community Survey

The first step is to create a list of all zip codes in Davidson County. We can do this by using another dataset that includes a comprehensive listing of zip codes by county and city. 

We start by using the lookup_code from the `tigris` package to get the fips codes for Davidson County in TN (Davidson is home to Vanderbilt).

```{r}
## Look up fips code for county
lookup_code("TN","Davidson")
```

```{r}
#storing this as objects for later use
state_fips<-"47"
county_stub<-"037"

#We can also combine using the `paste0` command
county_fips<-paste0(state_fips,county_stub)
```



```{r}
# Get and save dataset from the Census.gov website that matches all zip codes to cities, counties and states. 
county_to_zip<-read_csv("http://www2.census.gov/geo/docs/maps-data/data/rel/zcta_county_rel_10.txt")

save(county_to_zip,file="county_to_zip.Rdata")

#change all county names to lower case, a good standard practice in R
names(county_to_zip)<-tolower(names(county_to_zip))

#filter out just the zip codes our selected county, and selecting the zip, state and county columns only
county_to_zip<-county_to_zip%>%filter(state==state_fips,county==county_stub)%>%select(zcta5,state,county)

#create a list of zip codes in the county
ziplist<-county_to_zip$zcta5

#load the zip_codes data file that is in the noncensus package-- we will use this to get the city names
data(zip_codes)

#filter out just the names for the zip codes on our ziplist, and selecting the zip and city columns only
city_zip<-zip_codes%>%filter(zip%in%ziplist)%>%select(zip,city)

#Arrange in order
city_zip<-city_zip%>%arrange(as.numeric(zip))
```

Next, we'll turn to the American Community Survey. This includes a large number of tables (available here in excel file form:  https://www.census.gov/programs-surveys/acs/technical-documentation/summary-file-documentation.html) that cover many demographic and other characteristics of the population, down to the level of zip codes. We'll use the `acs` package to get two tables for the zip codes we're interested in: levels of education and income. We'll turn these tables into two variables: the proportion of the population with incomes above $75,000, and the proportion of the population with at least a bachelor's degree. 

The first step is to get the table from ACS. Below, I submit a request using my key to get table B15002, which contains information on education levels. 

```{r}
# Get your own key and save as my_acs_key.txt
my_acs_key<-readLines("my_acs_key.txt",warn = FALSE)
acs_key<-my_acs_key

# Or just paste it here.
#acs_key<-"XXX"

#List of tables: https://www.census.gov/programs-surveys/acs/technical-documentation/summary-file-documentation.html under, 1-year appendices
# b15002: education of pop over 25, by sex 
# b19001: household income over last 12 months

api.key.install(acs_key, file = "key.rda")

#this turns our list of zip codes into a geography file, which we will use to pull data
select_zip<-geo.make(zip.code=ziplist)

#creating a data table with data from 2014, from table B15002, with complete column names
county_educ=acs.fetch(geography=select_zip,
                      endyear=2014,
                      table.number="B15002",
                      col.names="pretty",verbose=T)

#This will display the column names
acs.colnames(county_educ) #This may take a LONG time
```

## Organizing ACS data

The trick with ACS data is organizing it in a way that's going to make sense. For us to get the proportion of individuals with a college degree or more, we're going to need to take the numbers of people who are in each of the various age levels for education, and then divide by the total number of people in the zip code. Below I include code to calculate the proportion of individuals in each zip code who have at least a bachelor's degree. 

```{r}
#Proportion of individuals at college or above = number with college degree/total number

#So we need columns 15, 16, 17, 18, 32, 33, 34, and 35 for the numerator, divided by the total in column 1

prop_coll_above<-divide.acs(numerator=(county_educ[,15]+
                                      county_educ[,16]+
                                      county_educ[,17]+
                                      county_educ[,18]+
                                      county_educ[,32]+
                                      county_educ[,33]+
                                      county_educ[,34]+
                                      county_educ[,35]),
                            denominator=county_educ[,1]
)
```


## Family Income Data
```{r}
#Repeating for the family income tablem 19001

#Note this data is from 2015
county_income<-acs.fetch(geography=select_zip, 
                        endyear = 2015,
                        table.number="B19001", 
                        col.names="pretty")

#This is display the column names from the income table.
acs.colnames(county_income) #This may take a LONG time.

#Proportion above 75k-- we will add together the 13, 14, 15, 16, and 17; then divide by the total in column 1
prop_above_75<-divide.acs(numerator=(county_income[,13]+
                            county_income[,14]+
                            county_income[,15]+
                            county_income[,16]+
                            county_income[,17]),
                          denominator=county_income[,1]
                          )

```


```{r}
                  
#Convert to tibble (or simplified dataframe)

county_df<-tibble(substr(geography(county_educ)[[1]],7,11), #the `substr` command pulls just the 5 digits of the zip code
                  as.numeric(estimate(prop_coll_above)),
                  as.numeric(estimate(prop_above_75)))

# Give it easy to use names; then save
names(county_df)<-c("zip","college_educ","income_75")
save(county_df,file="dav.RData")

head(county_df)
```

_Quick Exercise_ Pull table B23001 "Sex by Age by Employment Status for the Population 16 and over" from ACS. 

This resource is amazingly helpful. It means that with a list of zip codes you can get a huge amount of information about the area where the individual resides, including education, housing, income, medical care and other topics. 
